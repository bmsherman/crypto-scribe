
\documentclass[10pt]{article}
\usepackage{amsfonts,amsthm,amsmath,amssymb}
\usepackage{array}
\usepackage{epsfig}
\usepackage[
  margin=1.5cm,
  includefoot,
  footskip=30pt,
]{geometry}

%%% YOUR NAMES GO HERE %%%
\newcommand{\scribes}{Eric Atkinson, Ben Sherman}
%%% LECTURE NUMBER %%%
\newcommand{\lecnumber}{24}
%%% TITLE OF THE LECTURE %%%
\newcommand{\lectitle}{Cryptography when secrecy is hard to find (leakage resistance)}
%%% DATE OF THE LECTURE %%%
\newcommand{\thedate}{May 4, 2016}

\newcommand{\bit}{\{0,1\}}
\newcommand{\unif}[1]{\mathcal{U}\left( #1 \right)}
\newcommand{\Prob}[1]{\text{Pr}\left[ {#1} \right]}
\newcommand{\given}{\ |\ }
\newcommand{\suchthat}{\given}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bool}{\mathbb{B}}
\newcommand{\cat}{\ ||\ }
\newcommand{\then}{\ ;\ }

\begin{document}
\input{scribe-template-preamble.tex}

\handout{6.875J/18.425J Cryptography and Cryptanalysis}
{\thedate}
{Instructor: Shafi Goldwasser}
{Scribes: \scribes}
{Lecture \lecnumber: \lectitle}
{\lecnumber}

%%%% body goes in here %%%%
\section{Non-comprehensive Instructions}
\begin{itemize}
	\item Finish a first draft of the scribe notes by the day after lecture (i.e. Wednesday lecture $\rightarrow$ draft by Thursday night).
	\item Send us your source (.tex) files, and a compiled file as as a PDF. (By email: {\tt 6.875-ta-sp16@mit.edu})
\end{itemize}

\section{Modern theoretical cryptography}

\begin{itemize}
\item Computational notions
\item Surprising abilities
\item Reductionist security proofs from computational assumptions to secure primitives
\end{itemize}

So far in the class, the cryptographic protocols and constructions that we have considered have used the following assumptions:
\begin{itemize}
\item Algorithms are public
\item Private keys are perfectly secret and random
\item Cryptographic computations are opaque to an adversary
\end{itemize}

For instance, a secret key that is stored on one's computer cannot be observed by an adversary. However, the above assumptions do not hold in many real-life settings, because computations ``leak.''

When computations run on a computer, they consume power, and emit noise as well as radiation. If there's someone who can measure how much time the computation takes, how much heat the computer generates (such as by detecting changes in temperature in the nearby environment), this can give you information about the secret keys. 

It's possible to translate physical observations into attacks on the security of cryptographic schemes. Our proofs in the class so far have not taken into account these considerations.

Even if you don't compute at all, there are cold boot attacks. If your computer powered down, and someone gets a hold of it, before too long, they can look at the memory and recovery secret keys. History is maintained on the computer! Even if you thought you got rid of things, you may not have.

Garfunkle looked at erased disks and was able recover lots of forensic information.

Information about secret keys is available, sometimes do to re-use of secret keys across crypto applications. If someone has access to a lot of your cryptographic public keys, and if the private keys are correlated to each other (due to a shared source of randomness), it may be possible to break crypto.

Countermeasures:

In hardware: Shield things such as radiation or heat leakage. Using very weak devices you could get on eBay for cheap, he can break your scheme, just from the fact that you are grounding your computer. Could even break crypto just with a smartphone listening to the whirring of a computer's motherboard from 3 feet away.

Timing attack: looking at how much time RSA signing scheme to figure out the exponent by seeing how much time it takes for the signing to complete (since larger exponents take more time, done with repeated squaring).

In software: fixed timing, oblivious RAM, many heuristics

Can we use more theory in design time to battle reality? Can we do cryptography assuming NO secrecy? Maybe we don't need secret keys at all. Maybe everything can be public, and we can still do public key encryption, secure computation, and so on.

We can go a long way. We can't do away with all secrecy, but can reduce the amount of secrecy required.

Theoretical work:
- How to model these attacks?
- Modify scheme at design time
- Construct general compilers from classical secure protocols into ones secure against leaking attacks

These works are highly technical, so we're only getting a glimpse. But this stuff is of questionable benefit. We imagine that there's some fraction of leakage that can take place; suppose 90\% of the key is leaked. But if 95\% is leaked, you need to change your scheme.

Recipe: Model attacks, define security, implement crypto primitive.

Now, we will mathematically model side-channel attacks (computation time, power, etc.). Sometimes we will add secure hardware assumptions to cryptographic assumptions.

Challenges: don't increase computational assumptions, and minimize the use of secure hardware. You have to build secure hardware, believe it's secure. Nothing is 100\% satisfactory here. Obfuscation would be 100\% satisfactory, if it existed.

The new model:

Still have a PPT advesary $A$. But $A$ can also make measurements. $A$ can choose any function $L$ which is a function of the internal state of the computation. $L$ must come from some restricted class (if $L$ were the identity function, we could just steal the secret key directly).

The models differ:
- What does $L$ really have access to? The entire state of the computation? Only a certain amount of internal state? Only things that are in the cache?
- What complexity class does $L$ come from?
- Perhaps $L$ must be polynomial time, ``low depth''.
- How many measurements? Can measure once, or every once in a while?
- When is $L$ chosen? Before or after the public key is chosen?

ML: Global measurements
$L$ applies to the \emph{entire} memory

OC: Local measurements
$L$ can \emph{only} be applied to memory that is being applied in the current computation step

Bounded leakage: Can measure once, or a limited number of times

Continual leakage: You aren't bounded in how many times you can measure. BUT, you don't leak the entire memory, because you can erase things in your memory. So for instance, you can use the same public key, but keep changing secret keys, erasing previous ones, so you can never fully determine a single secret key.

What is the quality of information that $L$ leaks? Doesn't need to be perfect. Maybe it is just a random variable biased in the direction of the correct answer.

\section{Part 1: Bounded memory leakage (ML)}

How to design public key encryption where someone can measure the secret key a bounded number of times?

Memory retains its content after power is lost. Contents in RAM decay slowly; last for several minutes.

Can recover ``noisy''' keys. Say you're using RSA, and you have stored in memory the decryption exponent $d$ (where $d e = 1 \mod \varphi(n)$). You can' see $d$ exactly, but can see a noisy version of $d$. We want to deal with this kind of attack.

Update the notion of semantic security to take into account the adversary's new capabilities. We're going to change it as follows: There is semantic security against a $\lambda$-memory attack if for any leakage function $L(\cdot)$, the adversary can't find $m_0$ and $m_1$ that can distinguish between the encryptions.

The restriction is that $L : SK \to \bit^n$ such that $n = |L(SK)| < \lambda(|SK|)$. This forces that there is some entropy left in the secret key. In fact, this is just a strong as simply requiring that there is some entropy left in the secret key.

RSA variants: RSA is insecure theoretically with 50\% leakage, practically it was insecure with 25\% leakage?

Can we do it with an arbitrarily large fraction of leakage?

General randomized encoding of secret key: increases length of storage, decreases efficiency.

Public key encryption secure against memory leakage attacks.

\begin{theorem}
For every $\varepsilon > 0$, there exists a public key encryption scheme that can tolerate $\lambda = (1 - \varepsilon)n$ leakage attacks.
\end{theorem}
They use either lattice problems, Diffie-Hellman decisional intractability, or Quadratic Residuosity intractabilty.

What makes these schemes leakage resilient?

Let's imagine what a proof of security could look like. Old proofs of security: suppose we have a PK encryption scheme based on some hard problem. Suppose there's a leakage adversary, we want to show that we can solve the problem.

Start with a public key $pk$ for which it is a hard problem to find the secret key $sk$.

Adversary            Challenger
<- PK
-> L
<- L(SK)
-> ($m_0, m_1$)
Challenger flips random bit $b$
<- Enc($m_b$)
-> b'
Does (b == b')?

New paradigm: Instead of having one secret key, we will have one public key, and many secret keys for a single public key. Why is this helpful? Suppose that we did have this property. Exponentially many secret keys for a given single public key. Correctness: All of the secret keys can decrypt correctly. But, want to make it so that given one secret key, it should be hard computationally to find any other secret key.

Giving bounded leakage about any single secret key is not enough to point to it, so adversary with likely output another one. Then, if the adversary does succeed, he almost surely successfully outputs a different secret key from the one which the challenger does not already know, and so together they have solved the computationally hard problem of finding another secret key from one given secret key.

If an adversary could find a secret key, then we violate

\section{QR-based encryption scheme}


\textbf{Setup:} Choose a Blum integer $N$ such that no party knows the factorization. Let $\ell$ be a parameter for the amount of leakage.

\textbf{Key Generation:} Randomly sample  $sk = (s_0,\dots,s_\ell) \leftarrow \{0,1\}^\ell$. For $i \in [1,\ell]$, sample $g_i \leftarrow QR_N$. $pk = (g_1,\dots,g_\ell,h)$, where $h = \prod_{i=1}^\ell g_i^{-s_i}$.

\textbf{Encryption:} Randomly sample $r \leftarrow \mathbb{Z} \cap [0,N^2]$. For a message $m \in \{0,1\}$ and public key $(g_1,\dots,g_\ell,h)$, compute the ciphertext $c = (g_1^r,\dots,g_\ell^r,(-1)^m h^r)$. 

\textbf{Decryption:} Given the ciphertext $(c_1,\dots,c_\ell,c_0)$ and secret key $(s_1,\dots,s_\ell)$, compute $m = c_0 \prod_{i=1}^\ell c_i^{s_i}$.

Note that we chose $r$ from a set of size $N^2$, not $N$. The reason for this is ideally we would choose $r$ from a set the same size as $QR_N$, but we can't compute $|QR_N|$ without the factorization of $N$. Choosing $r$ from a set of size $N^2$ gets us close enough.

\textbf{Remark 1:} There are exponentially many secret keys which match the public key, for $\ell$ large enough. In fact, for the right choice of $\ell$, the $h$ will be very close to a uniform distribution; this follows from the leftover hash lemma. 

\begin{definition}
$H$ is a 2-universal hash family for $\mathcal{X} \rightarrow \mathcal{Y}$ if $\forall x,y \; \Pr_{h \leftarrow H}(h(x) = h(y)) = \frac{1}{|\mathcal{Y}|}$.
\end{definition}

\begin{lemma}
The leftover hash lemma. If $H$ is a 2-universal hash family on $\mathcal{X} \leftarrow \mathcal{Y}$, then for any $x \in \mathcal{X}$, $\mathcal{L} \in \mathcal{X} \rightarrow \mathcal{Y}$, $(h,h(x),L(x)) \approx (h,y,L(x))$, where $h \leftarrow H$ and $y \leftarrow \mathcal{Y}$.
\end{lemma}

The claim here is that the hash function family $h_{g_1,\dots,g_\ell}(x) = \prod_{i=1}^\ell g_i ^ {x_i}$ is 2-universal. This tells us that for sufficiently large $\ell$, $(g_1,\dots,g_\ell,h) \approx (g_1,\dots,g_\ell,U)$.

\begin{definition}
Min-entropy. For a probability distribution $X$, we can define the min-entropy as $H_1(X) = -\log \max_x \Pr(X = x)$. We say a random source is a $k$-source if $H_1(X) > k \iff \forall x \Pr(X = x) < 2^{-k}$.
\end{definition}

\begin{definition}
An extractor is a function $\mathrm{Ext}(x,r)$, where $x$ comes from a $k$-source, $r$ is a string of $d$ random bits, and $\mathrm{Ext}(x,r)$ is a string of bits of length $\ell$ that is indistinguishable from uniform.
\end{definition}

We can futher refine the definition of an extractor to require it to output nearly uniform bits even when the random seed is exposed. The leftover hash lemma states that we can build these strong extractors out of 2-universal hash functions.

\textbf{Remark 2:} We can view the secret key as an entropy source, where leaking more bits of the secret key will result in worse bounds on the entropy. We can then design our encryption algorithm to act as a randomness extractor from the secret key and a random seed $r$: $\mathrm{Enc} = \mathrm{Ext}(sk,r) \oplus m$. In our case, $h^r = \prod_{i=1}^\ell g_i^{s_i r}$ serves as the extractor, so $h^r$ is indistinguishable from uniform. All schemes which we know which are leakage-resilient make
use of extractors.

\section{Part 2: Only computation leaks}

Want to identify a component $H$ that computes some elementary function, where we can implement any cryptographic algorithm securely when the adversary can run side channel attacks on the entire computation except for $H$. In fact we want more. Even, we can say we let the adversary run the entire executions himself, except for $H$.

$H$ should be some universal component that can be plugged in to any algorithm. Also it should be simple.

This idea of $H$ also comes up in one-time programs. One-time programs are programs $P$ which can be run on a single input, but after it has been run once, it cannot be run again (particularly, can't be run on another input).

Problem: Hardware is not a black box.

We want this little box $H$ that does not leak to do very little computation (as little as possible). Rewrite programs in such a way such that we use $H$ as little as possible (ideally, we would not even need $H$ at all!)

What is $H$? $H$ is a one-time memory unit (OTM). $H$ has two pieces of information in it: $k_0$ and $k_1$. It allows you to read one key, but then will erase the other key. Therefore, no computation is ever done on the key which is erased.

\begin{theorem}
(Under fully homomorphic encryption) Can compile any $f$ into one-time programs for $f$ where program = software + $H$. Use Yao's garbled circuits, and for every input wire, we will use OTMs to store the keys.
\end{theorem}

In the context of leakage resilient, one-time programs enable executions of any cryptographic functionality such that the adversary learns nothing but the output even if all secrets that are computed on are fully leaked.

Turn executions of cryptographic algorithms into OTPs (one time programs). If we had a computation we want to run many times, even if someone is watching, I can prepare in advance representations of the computation, such that I can compute them online later on (analogy to the one time pad). E.G., if we want to be able to sign 100 messages in the presence of computation leaks, can prepare 100 OTPs in advance to accomplish this.

In leakage resilience setting, we don't need secure hardware. And we don't want to have to prepare things in advance. Can we prepare work in a bounded amount of time, and later execute an unbounded amount of executions?

The answer is that we have made some progress. Suppose we take the $H$ OTM memory cells, and we can encrypt their contents. We have garbled circuits, with the keys encrypted on the OTMs. Depending on whether the bit is $0, 1$, we take one of the keys from the OTM. Now when we need to decrypt the key from the OTM, we now need a secure leakage-resilient encryption algorithm for encrypting the keys that we put on the OTM.

But in the first part of the talk, we only did it a bounded number of times, and here we would need unbounded.


% % % You should probably leave the below alone % % %
\nocite{*}
\bibliographystyle{alpha}
\bibliography{lect-24-scribe}
\end{document}
